#!/usr/bin/env python
import argparse
import math
import os
import os.path
import re
import subprocess
import stat
import sys
import fnmatch
import errno

# setup defaults (let environment override)
PPN = os.environ.get("QBATCH_PPN", 8)
NODES = os.environ.get("QBATCH_NODES", 1)
SYSTEM = os.environ.get("QBATCH_SYSTEM", "pbs")
CORES = os.environ.get("QBATCH_CORES", PPN)
PE = os.environ.get("QBATCH_PE", "smp")
SCRIPT_FOLDER = os.environ.get("QBATCH_SCRIPT_FOLDER", ".scripts/")

# environment vars to ignore when copying the environment to the job script
IGNORE_ENV_VARS = ['PWD', 'SGE_TASK_ID', 'PBS_ARRAYID', 'ARRAY_IND']

PBS_HEADER_TEMPLATE = """
{shebang}
#PBS -l nodes={nodes}{highmem}:ppn={ppn}
#PBS -j oe
#PBS -o {logdir}
#PBS -d {workdir}
#PBS -N {job_name}
#PBS {o_array}
#PBS {o_walltime}
#PBS {o_dependencies}
#PBS {o_options}
{env}
{header_commands}
ARRAY_IND=PBS_ARRAYID
""".strip()

SGE_HEADER_TEMPLATE = """
{shebang}
#$ {ppn}
#$ -j y
#$ -o {logdir}
#$ -wd {workdir}
#$ -N {job_name}
#$ {o_array}
#$ {o_walltime}
#$ {o_dependencies}
#$ {o_options}
{env}
{header_commands}
ARRAY_IND=SGE_TASK_ID
""".strip()

LOCAL_TEMPLATE = """
{shebang}
{env}
{header_commands}
cd {workdir}
""".strip()

# Stolen from
# http://blog.endpoint.com/2015/01/getting-realtime-output-using-python.html


def run_command(command, logfile=None):
    process = subprocess.Popen(
        command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    if logfile:
        filehandle = open(logfile, 'w')
    while True:
        output = process.stdout.readline()
        if output == '' and process.poll() is not None:
            break
        if output and logfile:
            print output.strip()
            filehandle.write(output.strip())
            filehandle.write('\n')
        elif output:
            print output.strip()
    rc = process.poll()
    if logfile:
        filehandle.close()
    return rc


def mkdirp(*p):
    """Like mkdir -p"""
    path = os.path.join(*p)

    try:
        os.makedirs(path)
    except OSError as exc:
        if exc.errno == errno.EEXIST:
            pass
        else:
            raise
    return path


def valid_file(string):
    """Checks argument is a file that exists or is -"""
    if string != '-' and not os.path.isfile(string):
        raise argparse.ArgumentTypeError(
            "The file {} does not exist".format(string))
    return string


def positive_int(string):
    """Checks agument is a positive integer"""
    msg = "Must be a positive integer"

    try:
        value = int(string)
    except ValueError:
        raise argparse.ArgumentTypeError(msg)

    if value < 1:
        raise argparse.ArgumentTypeError(msg)
    return value


def int_or_percent(string):
    """Checks argument is an integer or integer percentage"""
    if not re.match("^([-+]?\d+|^\d+5)$", string):
        msg = "Must be an integer or positive integer percentage"
        raise argparse.ArgumentTypeError(msg)
    return string

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        usage="%(prog)s [options] <command_file>",
        description="""Submits a list of commands to a queueing system.
        The list of commands can be broken up into 'chunks' when submitted, so
        that the commands in each chunk run in parallel (using GNU parallel).
        The job script(s) generated by %(prog)s are stored in the folder
        {}""".format(SCRIPT_FOLDER),
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("command_file",
                        help="""An input file containing a list of commands
                        which will be submitted to the queueing system. A
                        'command' is simply a call to a command line program or
                        script. Use - to read the command list from STDIN.""",
                        type=valid_file)
    parser.add_argument("-t", "--walltime",
                        help="""Maximum walltime for an array job element or
                        individual job submitted""")
    parser.add_argument("-c", default=1,
                        help="""Number of commands from the command list that
                        are wrapped into each job""",
                        type=positive_int)
    parser.add_argument("-j", default=CORES, type=int_or_percent,
                        help="""Number of commands each job runs in parallel.
                        If the chunk size (-c) is smaller than -j then only
                        chunk size commands will run in parallel. This option
                        can also be expressed as a percentage (e.g. 100%%) of
                        the total available cores.""")
    parser.add_argument("-N", "--jobname", action="store",
                        help="""Override default job name generated from
                        command_file, or set name for STDIN jobs""")
    parser.add_argument("-w", "--workdir", default=os.getcwd(),
                        help="Job working directory")
    parser.add_argument("--logdir", action="store", default="{workdir}/logs",
                        help="""Directory to save store log files from batch
                        system or local processes""")
    parser.add_argument("-n", action="store_true",
                        help="Dry run; nothing is submitted or run")
    parser.add_argument("-v", action="store_true", help="Verbose output")

    group = parser.add_argument_group('advanced options')
    group.add_argument("-o", "--options", action="append", default=[],
                       help="""A string containing custom options passed
                       directly to the queuing system.  For example, --options
                       "-l vf=8G". This option can be given multiple times.""")
    group.add_argument("--header", action="append",
                       help="""A string that will be inserted verbatim at the
                       start of the script. This can be used to insert
                       commands that will be run once per job. This option can
                       be given multiple times.""")
    group.add_argument("--afterok_pattern", action="append",
                       help="""Wait for successful completion of job with
                       name(s) matching glob pattern before starting""")
    group.add_argument("--nodes", default=NODES, type=positive_int,
                       help="Nodes to request per job")
    group.add_argument("--ppn", default=PPN, type=positive_int,
                       help="""Number of cores each job requests aka
                       processors per node (ppn on PBS, parallel environment
                       number on SGE). Cores can be over subscribed if -j is
                       larger than --ppn (which is useful when making use of
                       hyper-threading)""")
    group.add_argument("--pe", default=PE,
                       help="""The SGE parallel environment to use if more than
                       one core is reqeuested (via --ppn) for each job.""")
    group.add_argument("--highmem", action="store_true",
                       help="(Scinet-only) Submit to high memory nodes")
    group.add_argument("-i", action="store_true",
                       help="Use individual jobs instead of an array job")
    group.add_argument("-b", default=SYSTEM,
                       choices=['pbs', 'sge', 'local'],
                       help="""The type of queueing system to use. 'pbs' and
                       'sge' both make calls to qsub to submit jobs. 'local'
                       runs the entire command list (without chunking)
                       directly.""")
    group.add_argument("--no-env", action="store_true",
                       help="""Do not copy the current environment into the job
                       script(s)""")
    args = parser.parse_args()

    command_file = args.command_file
    walltime = args.walltime
    options = args.options
    header_commands = args.header and '\n'.join(args.header) or ''
    workdir = args.workdir
    use_array = not args.i
    chunk_size = args.c
    ncores = args.j
    system = args.b
    dry_run = args.n
    nodes = args.nodes
    ppn = args.ppn
    afterok_pattern = args.afterok_pattern
    pe = args.pe
    highmem = args.highmem and ':m32g' or ''
    job_name = args.jobname
    logdir = args.logdir.format(workdir=workdir)
    copy_env = not args.no_env
    verbose = args.v

    mkdirp(logdir)

    # read in commands
    if command_file == '-':
        job_name = job_name or 'STDIN'
        task_list = sys.stdin.readlines()
    else:
        task_list = open(command_file).readlines()

    job_name = job_name or os.path.basename(command_file)

    # compute the number of jobs needed. This will be the number of elements in
    # the array job
    if len(task_list) == 0:
        print >>sys.stderr, "No jobs to submit, exiting"
        sys.exit()

    if system == 'local':
        use_array = False
        num_jobs = 1
        chunk_size = sys.maxsize
    else:
        num_jobs = int(math.ceil(len(task_list) / float(chunk_size)))

    # copy the current environment
    env = ''
    if copy_env:
        env = '\n'.join(['export {}="{}"'.format(k, v.replace('"', r'\"')) for
                         k, v in os.environ.items()
                         if k not in IGNORE_ENV_VARS])
        env = env.replace("$", "$$")
        env = "# -- start copied env\n{}\n# -- end copied env".format(env)

    # make script header
    shebang = '#!/bin/bash'

    if system == 'pbs':
        matching_jobids = []
        if afterok_pattern:
            try:
                o = subprocess.check_output('pbs_jobnames')
            except subprocess.CalledProcessError as e:
                sys.exit(e)
            for row in o.strip().split("\n"):
                if not row.strip():  # skip empty lines
                    continue
                jobid, name = row.split()
                for pattern in afterok_pattern:
                    if fnmatch.fnmatch(name, pattern):
                        matching_jobids.append(jobid)

        o_array = use_array and '-t 1-{}'.format(num_jobs) or ''
        o_walltime = walltime and "-l {}".format(walltime) or ''
        o_dependencies = matching_jobids and \
            ('-W depend=' + (use_array and 'afterokarray:' or 'afterok:') +
                ':'.join(matching_jobids)) or ''
        o_options = '\n#PBS '.join(options)
        header = PBS_HEADER_TEMPLATE.format(**vars())

    elif system == 'sge':
        ppn = (ppn > 1) and '-pe {} {}'.format(pe, ppn) or ''
        o_array = use_array and '-t 1-{}'.format(num_jobs) or ''
        o_walltime = walltime and "#$ -h_rt={}".format(walltime) or ''
        o_dependencies = afterok_pattern and '-hold_jid ' + \
            afterok_pattern or ''
        o_options = '\n#$ '.join(options)
        header = SGE_HEADER_TEMPLATE.format(**vars())

    elif system == 'local':
        header = LOCAL_TEMPLATE.format(**vars())

    # emit job scripts
    job_scripts = []
    mkdirp(SCRIPT_FOLDER)
    if use_array:
        script_lines = [
            header,
            'CHUNK_SIZE={}'.format(chunk_size),
            'CORES={}'.format(ncores),
            'sed -n "$(( (${ARRAY_IND} - 1) * ${CHUNK_SIZE} + 1 )),'
            '+$(( ${CHUNK_SIZE} - 1 ))p" << EOF | parallel -j${CORES}',
            ''.join(task_list),
            'EOF']

        scriptfile = os.path.join(SCRIPT_FOLDER, job_name + ".array")
        script = open(scriptfile, 'w')
        script.write('\n'.join(script_lines))
        script.close()
        job_scripts.append(scriptfile)
    else:
        for chunk in range(num_jobs):
            scriptfile = os.path.join(
                SCRIPT_FOLDER, "{}.{}".format(job_name, chunk))
            script_lines = [
                header,
                'CORES={}'.format(ncores),
                "parallel -j${CORES} << EOF",
                ''.join(task_list[chunk * chunk_size:chunk *
                                  chunk_size + chunk_size]),
                'EOF']
            script = open(scriptfile, "w")
            script.write('\n'.join(script_lines))
            script.close()
            job_scripts.append(scriptfile)

    # execute the job script(s)
    for script in job_scripts:
        os.chmod(script, os.stat(script).st_mode | stat.S_IXUSR)
        if dry_run:
            continue
        if system == 'sge' or system == 'pbs':
            return_code = subprocess.call(['qsub', script])
            if return_code:
                sys.exit("qsub call returned error")
        else:
            logfile = "{}/{}.log".format(logdir, job_name)
            if verbose:
                print("Launching jobscript. Output to {}".format(logfile))
            run_command(script, logfile=logfile)
