#!/usr/bin/env python
import argparse
import math
import os
import os.path
import subprocess
import stat
import sys
import fnmatch
import errno

# setup defaults (let environment override)
PPN = os.environ.get("QBATCH_PPN", 12)
NODES = os.environ.get("QBATCH_NODES", 1)
SYSTEM = os.environ.get("QBATCH_SYSTEM", "pbs")
CORES = os.environ.get("QBATCH_CORES", PPN)

#Stolen from http://blog.endpoint.com/2015/01/getting-realtime-output-using-python.html
def run_command(command, logfile=None):
    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    if logfile:
        filehandle = open(logfile, 'w')
    while True:
        output = process.stdout.readline()
        if output == '' and process.poll() is not None:
            break
        if output and logfile:
            print output.strip()
            filehandle.write(output.strip())
            filehandle.write('\n')
        elif output:
            print output.strip()
    rc = process.poll()
    if logfile:
        filehandle.close()
    return rc

def mkdirp(*p):
    """Like mkdir -p"""
    path = os.path.join(*p)

    try:
        os.makedirs(path)
    except OSError as exc:
        if exc.errno == errno.EEXIST:
            pass
        else: raise
    return path

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Submit commands to a queueing system",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("command_file",
                        help="An input file containing a list of commands. Use - for STDIN")
    parser.add_argument("-t", "--walltime", help='Maximum walltime for a job.')
    parser.add_argument("-o", "--options", action="append", default=[],
                        help='A string containing custom options passed directly to the queuing system.  For example, --options "-l vf=8G". This option can be given multiple times.')
    parser.add_argument("--header", action="append", default=[],
                        help="A string that will be inserted verbatim at the start of the script. This can be used to insert commands that will be run once per job. This option can be given multiple times.")
    parser.add_argument("-w", default=os.getcwd(),
                        help="Job working directory")
    parser.add_argument("-i", action="store_true",
                        help="Use individual jobs instead of an array job")
    parser.add_argument("-c", default=1, type=int,
                        help="Number of input lines to chunk into a single job")
    parser.add_argument("-j", default=CORES, type=int,
                        help="Number of commands to run in parallel per job")
    parser.add_argument("-b", default=SYSTEM,
                        choices=['pbs', 'sge', 'local'], help="Queueing system to use")
    parser.add_argument("--nodes", default=NODES, type=int,
                        help="Nodes to request per job")
    parser.add_argument("--ppn", default=PPN, type=int,
                        help="Processors per node (ppn on PBS, parallel environment number on SGE)")
    parser.add_argument("--highmem", action="store_true",
                        help="(Scinet-only) Submit to high memory nodes")
    parser.add_argument("-n", action="store_true",
                        help="Dry run; nothing is submitted or run")
    parser.add_argument("--afterok_pattern", action="append",
                        help="Wait for successful completion of job with name(s) matching glob pattern before starting")
    parser.add_argument("--jobname", action="store",
                        help="Override default job name generated from command_file, or set name for STDIN jobs")
    parser.add_argument("--logdir", action="store", default=os.getcwd() + "/logs",
                        help="Directory to save store log files from batch system or local processes")

    args = parser.parse_args()

    command_file = args.command_file
    walltime = args.walltime
    options = args.options
    header_commands = args.header
    job_workdir = args.w
    use_array = not args.i
    chunk_size = args.c
    ncores = args.j
    system = args.b
    dry_run = args.n
    nodes = args.nodes
    ppn = args.ppn
    afterok_pattern = args.afterok_pattern
    highmem = args.highmem
    job_name = args.jobname
    logdir = args.logdir
    mkdirp(logdir)

    # read in commands
    if command_file == '-':
        job_name = job_name or 'STDIN'
        task_list = sys.stdin.readlines()
    else:
        task_list = open(command_file).readlines()

    job_name = job_name or os.path.basename(command_file)

    # compute the number of jobs needed. This will be the number of elements in
    # the array job

    if len(task_list) == 0:
        print >>sys.stderr, "No jobs to submit, exiting"
        sys.exit()

    if system == 'local':
        use_array = False
        num_jobs = 1
        chunk_size= sys.maxsize
    else:
        num_jobs = int(math.ceil(len(task_list) / float(chunk_size)))

    # make script header
    shebang = '#!/bin/bash -l'

    if system == 'pbs':
        matching_jobids = []
        if afterok_pattern:
            o = subprocess.check_output('pbs_jobnames')
            for row in o.strip().split("\n"):
                if not row.strip(): # skip empty lines
                    continue
                jobid, name = row.split()
                for pattern in afterok_pattern:
                    if fnmatch.fnmatch(name, pattern):
                        matching_jobids.append(jobid)

        header = '\n'.join([
            "{shebang}",
            "#PBS -l nodes={nodes}{highmem}:ppn={ppn}",
            "#PBS -V",
            "#PBS -j oe",
            "#PBS -o {logdir}",
            "#PBS -d {workdir}",
            "#PBS -N {jobname}",
            "#PBS {array}",
            "#PBS {options}",
            "{walltime}",
            "{dependencies}",
            "{header_commands}",
            "ARRAY_IND=PBS_ARRAYID"]).format(
            nodes=nodes,
            highmem=highmem and ':m32g' or '',
            ppn=ppn,
            shebang=shebang,
            logdir=logdir,
            workdir=job_workdir,
            jobname=job_name,
            options='\n#PBS '.join(options),
            walltime=walltime and "#PBS -l {}".format(walltime) or '',
            header_commands='\n'.join(header_commands),
            array=use_array and '-t 1-' + str(num_jobs) or '',
            dependencies=matching_jobids and ('#PBS -W depend=' + (use_array and 'afterokarray:' or 'afterok:') + ':'.join(matching_jobids)) or '')
    elif system == 'sge':

        header = '\n'.join([
            "{shebang}",
            "{ppn}",
            "#$ -V",
            "#$ -j y",
            "#$ -o {logdir}",
            "#$ -wd {workdir}",
            "#$ -N {jobname}",
            "#$ {array}",
            "#$ {options}",
            "{walltime}",
            "{dependencies}",
            "{header_commands}",
            "ARRAY_IND=SGE_TASK_ID"]).format(
            ppn=(ppn > 1) and '#$ -pe smp ' + str(ppn) or '',
            shebang=shebang,
            logdir=logdir,
            workdir=job_workdir,
            jobname=job_name,
            options='\n#$ '.join(options),
            walltime=walltime and "#$ -h_rt={}".format(walltime) or '',
            header_commands='\n'.join(header_commands),
            array=use_array and '-t 1-' + str(num_jobs) or '',
            dependencies=afterok_pattern and '#$ -hold_jid ' + afterok_pattern or '')
    elif system == 'local':

        header = '\n'.join([
            "{shebang}",
            "cd {workdir}"]).format(
            shebang=shebang,
            workdir=job_workdir)

    # emit job scripts
    job_scripts = []
    mkdirp(".scripts")
    if use_array:
        script_lines = [
            header,
            'sed -n "$(( (${{ARRAY_IND}}-1)*{size}+1)),+$(({size}-1))p" '
            '<< EOF | parallel -j{ncores} '.format(
                ncores=ncores, size=chunk_size),
            ''.join(task_list),
            'EOF']

        scriptfile = ".scripts/" + job_name + ".array"
        script = open(scriptfile, 'w')
        script.write('\n'.join(script_lines))
        script.close()
        job_scripts.append(scriptfile)
    else:
        for chunk in range(num_jobs):
            scriptfile = ".scripts/" + "{}.{}".format(job_name, chunk)
            script_lines = [
                header,
                "parallel -j{} << EOF".format(ncores),
                ''.join(task_list[chunk * chunk_size:chunk *
                                  chunk_size + chunk_size]),
                'EOF']
            script = open(scriptfile, "w")
            script.write('\n'.join(script_lines))
            script.close()
            job_scripts.append(scriptfile)

    # execute the job script(s)
    for script in job_scripts:
        os.chmod(script, os.stat(script).st_mode | stat.S_IXUSR)
        if dry_run:
            continue
        if system == 'sge' or system == 'pbs':
            return_code = subprocess.call(['qsub', script])
            if return_code:
                sys.exit("qsub call returned error")
        else:
            print "Launching Local Jobscript, saving output to {}/{}.log".format(logdir, job_name)
            run_command(script, logfile="{}/{}.log".format(logdir, job_name))
